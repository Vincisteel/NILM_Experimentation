{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to carry out your own NILM experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NILM experiment will be  on nilmtk.api as nilmtk-contrib does."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, you should import **nilmtk.api** and **specific algorithms from nilmtk.disaggregate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T08:02:22.547498Z",
     "start_time": "2024-03-14T08:02:09.250517Z"
    }
   },
   "outputs": [],
   "source": [
    "from nilmtk.api import API\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from nilmtk.disaggregate import SGN\n",
    "from nilmtk.disaggregate import DAE\n",
    "from nilmtk.disaggregate import Seq2Point\n",
    "from nilmtk.disaggregate import BiLSTM\n",
    "from nilmtk.disaggregate import EnerGAN\n",
    "from nilmtk.disaggregate import Seq2Seq\n",
    "from nilmtk.disaggregate import AttentionCNN\n",
    "\n",
    "from nilmtk.disaggregate import MUL_DAE\n",
    "from nilmtk.disaggregate import MUL_BiLSTM\n",
    "from nilmtk.disaggregate import MUL_Seq2Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert the eco data\n",
    "from nilmtk.dataset_converters import convert_eco\n",
    "\n",
    "# Specify the timezone as a string, e.g., 'Europe/Zurich'\n",
    "timezone = 'Europe/Zurich'\n",
    "\n",
    "convert_eco(r'C:\\Users\\char\\Documents\\GitHub\\NeuralNILM_Pytorch\\data\\eco', r'C:\\Users\\char\\Documents\\GitHub\\NeuralNILM_Pytorch\\data_hdf5\\eco\\data_cleaned.h5',timezone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-06T12:56:55.421683900Z",
     "start_time": "2024-03-06T12:41:16.007485100Z"
    }
   },
   "outputs": [],
   "source": [
    "#Convert the refit data\n",
    "from nilmtk.dataset_converters import convert_refit\n",
    "\n",
    "convert_refit(r'C:\\Users\\char\\Documents\\GitHub\\NeuralNILM_Pytorch\\data\\refit\\RAW_DATA_CLEAN', r'C:\\Users\\char\\Documents\\GitHub\\NeuralNILM_Pytorch\\data_hdf5\\refit\\data_refit.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata information regarding appliances in each building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, specify the **experiment configuration** as described in annotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T08:02:22.995108Z",
     "start_time": "2024-03-14T08:02:22.550739Z"
    }
   },
   "outputs": [],
   "source": [
    "#Data Exploration\n",
    "from nilmtk import DataSet\n",
    "import nilmtk\n",
    "from nilmtk.utils import print_dict\n",
    "\n",
    "nilmtk.Appliance.allow_synonyms = False\n",
    "\n",
    "refit = DataSet(r'C:\\Users\\char\\Documents\\GitHub\\NeuralNILM_Pytorch\\data_hdf5\\refit\\data_refit.h5')\n",
    "\n",
    "elec = refit.buildings[8].elec\n",
    "elec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the training data (REFIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-08T10:56:51.527810500Z",
     "start_time": "2024-03-08T10:56:40.829872300Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Function to read the appliance names mapping from the HOUSES_Labels.txt\n",
    "def read_appliance_mappings(filepath):\n",
    "    # Read the mapping from a file\n",
    "    mappings_df = pd.read_csv(filepath, index_col='House_id')\n",
    "    # Create a dictionary to hold mappings for each house\n",
    "    appliance_mappings = {}\n",
    "    for index, row in mappings_df.iterrows():\n",
    "        # Iterate over each appliance and add to the house's mapping\n",
    "        appliance_mappings[index] = {\n",
    "            f'Appliance{i}': row[f'Appliance{i}']\n",
    "            for i in range(1, 10) # Assuming there are 9 appliances max\n",
    "            if pd.notnull(row[f'Appliance{i}'])\n",
    "        }\n",
    "    return appliance_mappings\n",
    "\n",
    "# Load the appliance mappings\n",
    "appliance_mappings = read_appliance_mappings(r'C:\\Users\\char\\Documents\\GitHub\\NeuralNILM_Pytorch\\data\\refit\\RAW_DATA_CLEAN\\HOUSES_Labels')\n",
    "\n",
    "# Function to plot for a specific house_id\n",
    "def plot_house(house_id, appliance_mappings, data_filepath):\n",
    "    # Load the CSV file, assuming 'Time' is your datetime column\n",
    "    df = pd.read_csv(data_filepath)\n",
    "\n",
    "    # Convert the 'Time' column to datetime and set as index\n",
    "    df['Time'] = pd.to_datetime(df['Time'])\n",
    "    df.set_index('Time', inplace=True)\n",
    "\n",
    "    # Get the mapping for the specified house_id\n",
    "    appliance_mapping = appliance_mappings[house_id]\n",
    "\n",
    "    # Downsample the data to a reasonable frequency, e.g., hourly means\n",
    "    df_resampled = df.resample('H').mean()\n",
    "\n",
    "    # Number of appliances plus one for the aggregate data\n",
    "    num_plots = len(appliance_mapping) + 1\n",
    "\n",
    "    # Create a subplot for each appliance plus the aggregate data\n",
    "    fig = make_subplots(rows=num_plots, cols=1, shared_xaxes=False, vertical_spacing=0.03,\n",
    "                        subplot_titles=['Aggregate'] + list(appliance_mapping.values()))\n",
    "\n",
    "    # Add the 'Aggregate' line chart\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=df_resampled.index, y=df_resampled['Aggregate'], mode='lines', name='Aggregate'),\n",
    "        row=1, col=1\n",
    "    )\n",
    "\n",
    "    # Add traces for each appliance\n",
    "    row_index = 2\n",
    "    for key, value in appliance_mapping.items():\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=df_resampled.index, y=df_resampled[key], mode='lines', name=value),\n",
    "            row=row_index, col=1\n",
    "        )\n",
    "        row_index += 1\n",
    "\n",
    "    # Update layout for the figure\n",
    "    fig.update_layout(\n",
    "        template='plotly_dark',\n",
    "        font=dict(family=\"Arial\", size=12, color='white'),\n",
    "        height=200 * num_plots,\n",
    "        width=1000,\n",
    "        title_text=f\"Energy Consumption Visualization for House {house_id}\",\n",
    "        showlegend=True\n",
    "    )\n",
    "\n",
    "    # Update x-axis and y-axis titles\n",
    "    for i in range(num_plots):\n",
    "        axis_y = f'yaxis{i+1}' if i > 0 else 'yaxis'        \n",
    "        fig['layout'][axis_y].update(title='Power (W)')\n",
    "\n",
    "    # Show the interactive plot\n",
    "    fig.show()\n",
    "\n",
    "# Usage\n",
    "house_id = 8  # Replace with your house_id\n",
    "data_filepath = r'C:\\Users\\char\\Documents\\GitHub\\NeuralNILM_Pytorch\\data\\refit\\RAW_DATA_CLEAN\\CLEAN_House'+str(house_id)+'.csv'\n",
    "plot_house(house_id, appliance_mappings, data_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the testing data (ECO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T12:32:50.990856400Z",
     "start_time": "2024-03-12T12:32:07.910045300Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "def append_smart_meter_data(directory):\n",
    "    # Define the column names based on the provided information\n",
    "    col_names = [\n",
    "        'powerallphases', 'powerl1', 'powerl2', 'powerl3', 'currentneutral',\n",
    "        'currentl1', 'currentl2', 'currentl3', 'voltagel1', 'voltagel2',\n",
    "        'voltagel3', 'phaseanglevoltagel2l1', 'phaseanglevoltagel3l1',\n",
    "        'phaseanglecurrentvoltagel1', 'phaseanglecurrentvoltagel2',\n",
    "        'phaseanglecurrentvoltagel3'\n",
    "    ]\n",
    "\n",
    "    # Initialize an empty DataFrame to hold all the data\n",
    "    all_data = pd.DataFrame()\n",
    "\n",
    "    # List all files in the directory\n",
    "    files = os.listdir(directory)\n",
    "    csv_files = [f for f in files if f.endswith('.csv')]\n",
    "\n",
    "    # Sort files to maintain chronological order\n",
    "    csv_files.sort()\n",
    "\n",
    "    # Read each file and append the data to the all_data DataFrame\n",
    "    for file_name in csv_files:\n",
    "        file_path = os.path.join(directory, file_name)\n",
    "        \n",
    "        # Extract the date from the file name (assuming the format is YYYY-MM-DD.csv)\n",
    "        date_str = file_name.split('.')[0]\n",
    "        date = datetime.strptime(date_str, '%Y-%m-%d')\n",
    "\n",
    "        # Read the CSV file without headers\n",
    "        daily_data_smart = pd.read_csv(file_path, header=None, names=col_names)\n",
    "\n",
    "        # Create a timestamp for each second of the day\n",
    "        timestamps = pd.date_range(date, periods=len(daily_data_smart), freq='S')\n",
    "\n",
    "        # Assign the timestamp as the index and create a 'timestamp' column\n",
    "        daily_data_smart.index = timestamps\n",
    "        daily_data_smart['timestamp'] = timestamps\n",
    "\n",
    "        # Resample the data to hourly averages and keep the first timestamp of the hour\n",
    "        daily_data_smart = daily_data_smart.resample('H').agg({**{col: 'mean' for col in col_names}, 'timestamp': 'first'})\n",
    "\n",
    "        # Append the daily data to the all_data DataFrame\n",
    "        all_data = pd.concat([all_data, daily_data_smart])\n",
    "\n",
    "    # Reset index in the final DataFrame\n",
    "    all_data.reset_index(drop=True, inplace=True)\n",
    "    return all_data\n",
    "\n",
    "# Updated appliance mapping function that reads from a .txt file\n",
    "def map_appliance_data_from_txt(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        plug_data_str = file.read()\n",
    "    return map_appliance_data(plug_data_str)\n",
    "\n",
    "# Function to parse the appliance data\n",
    "def map_appliance_data(plug_data_str):\n",
    "    lines = plug_data_str.strip().split('\\n')\n",
    "    appliance_data = {}\n",
    "    for line in lines:\n",
    "        match = re.match(r\"(\\d+):\\s+([A-Za-z\\s]+)\\(no\\. days:\\s+(\\d+),\\s+coverage:\\s+(\\d+\\.\\d+)%\\)\", line)\n",
    "        if match:\n",
    "            appliance_number = match.group(1)\n",
    "            appliance_type = match.group(2)\n",
    "            no_days = int(match.group(3))\n",
    "            coverage = float(match.group(4))\n",
    "            appliance_data[appliance_number] = {\n",
    "                \"device\": appliance_type.strip(),\n",
    "                \"no_days\": no_days,\n",
    "                \"coverage\": coverage\n",
    "            }\n",
    "    return appliance_data\n",
    "\n",
    "# Function to read all appliance data from a building\n",
    "def read_all_appliance_data(building_path, num_appliances):\n",
    "    appliance_data = {}\n",
    "    for i in range(1, num_appliances + 1):\n",
    "        appliance_number = str(i).zfill(2)  # Assuming appliance number is zero-padded\n",
    "        appliance_path = os.path.join(building_path, appliance_number)\n",
    "        appliance_data[appliance_number] = read_appliance_data(appliance_path)\n",
    "    return appliance_data\n",
    "\n",
    "# Function to read appliance level data from .csv files and add timestamps\n",
    "def read_appliance_data(appliance_path):\n",
    "    file_list = [f for f in os.listdir(appliance_path) if f.endswith('.csv')]\n",
    "    data_frames = []\n",
    "    for file_name in file_list:\n",
    "        # Extract the date from the file name (assuming the format is YYYY-MM-DD.csv)\n",
    "        date_str = file_name.split('.')[0]\n",
    "        date = datetime.strptime(date_str, '%Y-%m-%d')\n",
    "\n",
    "        # Read the CSV file without headers\n",
    "        daily_data = pd.read_csv(os.path.join(appliance_path, file_name), header=None)\n",
    "        \n",
    "        # Create a timestamp for each second of the day\n",
    "        timestamps = pd.date_range(date, periods=86400, freq='S')\n",
    "\n",
    "        # Give names to the columns\n",
    "        daily_data.columns = ['power']\n",
    "        daily_data['timestamp'] = timestamps\n",
    "\n",
    "        # Set the timestamp as the index\n",
    "        daily_data.set_index('timestamp', inplace=True)\n",
    "\n",
    "        # Resample the data to hourly averages\n",
    "        daily_data = daily_data.resample('H').mean()\n",
    "\n",
    "        # Append the daily data to the list of dataframes\n",
    "        data_frames.append(daily_data)\n",
    "\n",
    "    # Concatenate all daily dataframes\n",
    "    appliance_data = pd.concat(data_frames)\n",
    "    \n",
    "    # Reset index in the final DataFrame if you want the timestamp as a column\n",
    "    appliance_data.reset_index(inplace=True)\n",
    "    \n",
    "    return appliance_data\n",
    "\n",
    "# Function to plot aggregated and appliance data\n",
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "def plot_aggregated_and_appliance_data(aggregated_data, appliance_data_dict, appliance_mapping):\n",
    "    # Number of appliances\n",
    "    num_appliances = len(appliance_data_dict)\n",
    "    \n",
    "    # Create subplots: one for aggregated data and one for each appliance\n",
    "    fig = make_subplots(rows=num_appliances + 1, cols=1, \n",
    "                        subplot_titles=['Aggregated Data'] + [f'{v[\"device\"]}' for k, v in appliance_mapping.items()],\n",
    "                        shared_xaxes=False, vertical_spacing=0.03)\n",
    "    \n",
    "    # Extract device names into a simple mapping dictionary\n",
    "    device_name_mapping = {key: value['device'] for key, value in appliance_mapping.items()}\n",
    "\n",
    "    # Add aggregated data to the first subplot\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=aggregated_data['timestamp'], y=aggregated_data['powerallphases'], \n",
    "                   name='Aggregated', mode='lines'),\n",
    "        row=1, col=1\n",
    "    )\n",
    "\n",
    "    # Add appliance data to their respective subplots\n",
    "    appliance_row = 2\n",
    "    for appliance_number, data in appliance_data_dict.items():\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=data.iloc[:, 0], y=data.iloc[:, 1], \n",
    "                       name=device_name_mapping[appliance_number], mode='lines'),\n",
    "            row=appliance_row, col=1\n",
    "        )\n",
    "        appliance_row += 1  # Move to the next subplot for the next appliance\n",
    "\n",
    "    # Update xaxis properties for the last subplot\n",
    "    fig.update_xaxes(title_text=\"Time\", row=num_appliances + 1, col=1)\n",
    "\n",
    "    # Update yaxis properties for all subplots\n",
    "    for r in range(1, num_appliances + 2):\n",
    "        fig.update_yaxes(title_text=\"Power (Watts)\", row=r, col=1)\n",
    "\n",
    "    # Update layout to include 'Arial' font and adjust the template\n",
    "    fig.update_layout(\n",
    "        height=200 * (num_appliances + 1),  # adjust the height based on the number of appliances\n",
    "        width=1000,\n",
    "        title_text=\"Aggregated and Appliance Power Data\",\n",
    "        showlegend=True,\n",
    "        template=\"plotly_dark\",\n",
    "        font=dict(family=\"Arial\", size=12, color=\"white\")\n",
    "    )\n",
    "\n",
    "    # Show figure\n",
    "    fig.show()\n",
    "\n",
    "# Example usage:\n",
    "building_number = \"01\"\n",
    "metadata_path = os.path.join(r\"C:\\Users\\char\\Documents\\GitHub\\NeuralNILM_Pytorch\\data\\eco\", building_number + '_doc.txt')\n",
    "\n",
    "# Map appliance data using the updated function\n",
    "appliance_mapping = map_appliance_data_from_txt(metadata_path)\n",
    "\n",
    "# Get the number of appliances from the last appliance mapped\n",
    "num_appliances = int(list(appliance_mapping.keys())[-1])\n",
    "\n",
    "# Path to the appliance data directory\n",
    "building_appliance_data_path = os.path.join(r\"C:\\Users\\char\\Documents\\GitHub\\NeuralNILM_Pytorch\\data\\eco\", building_number + '_plugs_csv', building_number)\n",
    "\n",
    "# Path to the smart meter data directory\n",
    "smart_meter_data_path = os.path.join(r\"C:\\Users\\char\\Documents\\GitHub\\NeuralNILM_Pytorch\\data\\eco\\01_sm_csv\", building_number)\n",
    "\n",
    "# Read smart_meter_data data\n",
    "smart_meter_data = append_smart_meter_data(smart_meter_data_path)\n",
    "\n",
    "# Read all appliance data\n",
    "all_appliance_data = read_all_appliance_data(building_appliance_data_path, num_appliances)\n",
    "\n",
    "# Plot the data\n",
    "plot_aggregated_and_appliance_data(smart_meter_data, all_appliance_data,  appliance_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-15T14:00:06.606619Z",
     "start_time": "2024-03-15T13:54:16.790425Z"
    }
   },
   "outputs": [],
   "source": [
    "e = {\n",
    "  # Specify power type, sample rate and disaggregated appliance\n",
    "  'power': {\n",
    "    'mains': ['active'],\n",
    "    'appliance': ['active']\n",
    "  },\n",
    "  'sample_rate': 900,\n",
    "  'appliances': {'washing machine'},\n",
    "  # Universally no pre-training\n",
    "  'pre_trained':False,\n",
    "  # Specify algorithm hyperparameters\n",
    "  'methods':{\"Seq2Point\":Seq2Point({'n_epochs':2,'batch_size':512}), \"SGN\":SGN({'n_epochs':2,'batch_size':512}), \"DAE\":DAE({'n_epochs':2,'batch_size':512}), \"BiLSTM\":BiLSTM({'n_epochs':2,'batch_size':512})},\n",
    "  # Specify train and test data\n",
    " 'train': {\n",
    "    'datasets':{\n",
    "      'refit': {\n",
    "        'path': r'C:\\Users\\char\\Documents\\GitHub\\NeuralNILM_Pytorch\\data_hdf5\\refit\\data_refit.h5',\n",
    "        'buildings': {\n",
    "              8: {\n",
    "                    'start_time': '2014-03-07',\n",
    "                    'end_time': '2015-03-06'\n",
    "              }\n",
    "          }\n",
    "        },      \n",
    "    }\n",
    "  },\n",
    "  'test': {\n",
    "    'datasets':{\n",
    "      'refit': {\n",
    "        'path': r'C:\\Users\\char\\Documents\\GitHub\\NeuralNILM_Pytorch\\data_hdf5\\refit\\data_refit.h5',\n",
    "        'buildings': {\n",
    "              8: {\n",
    "                    'start_time': '2015-03-07',\n",
    "                    'end_time': '2015-04-06'\n",
    "              }\n",
    "          }\n",
    "        },\n",
    "        \n",
    "  },\n",
    "  # Specify evaluation metrics\n",
    "  'metrics': ['mae', 'f1score', 'recall', 'precision', 'nep' , 'omae', 'MCC', ]\n",
    "  }\n",
    "}\n",
    "\n",
    "API(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above results, we can know the **inferring and training time consumption** of specific neural network on the given dataset as well as **emory usage**. **Classification metrics(namely recall, precision, and MCC)** and **regression metrics(mae, omae, nep)** will also be reported.And **detailed energy disaggregation result and ground truth** can be found in .csv file under the same folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-15T16:06:29.106740Z",
     "start_time": "2024-03-15T16:06:28.888040Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Import the data from Excel\n",
    "file_path = r'C:\\Users\\char\\Documents\\GitHub\\NeuralNILM_Pytorch\\Results_v1.4.xlsx'  # Make sure to use the correct path to your Excel file\n",
    "\n",
    "# Load data from each sheet\n",
    "df_mae = pd.read_excel(file_path, sheet_name='mae', engine='openpyxl')\n",
    "df_f1 = pd.read_excel(file_path, sheet_name='f1', engine='openpyxl')\n",
    "df_nep = pd.read_excel(file_path, sheet_name='nep', engine='openpyxl')\n",
    "\n",
    "# Assuming 'Validation Type' column exists and contains 'Intra' or 'Cross'\n",
    "# Filter the dataframes based on the type of validation\n",
    "df_mae_intra = df_mae[df_mae['Type'] == 1]\n",
    "df_f1_intra = df_f1[df_f1['Type'] == 1]\n",
    "df_nep_intra = df_nep[df_nep['Type'] == 1]\n",
    "\n",
    "df_mae_cross = df_mae[df_mae['Type'] == 2]\n",
    "df_f1_cross = df_f1[df_f1['Type'] == 2]\n",
    "df_nep_cross = df_nep[df_nep['Type'] == 2]\n",
    "\n",
    "# Assuming the structure of the sheets is the same, we can create a function\n",
    "def create_metric_subplot(df_intra, df_cross, metric_name, appliances, algorithms, show_legend=False):\n",
    "    traces = []\n",
    "    for appliance in appliances:\n",
    "        intra_data = df_intra[df_intra['Appliance'] == appliance]\n",
    "        cross_data = df_cross[df_cross['Appliance'] == appliance]\n",
    "        for algo in algorithms:\n",
    "            color = algorithm_colors.get(algo, 'rgba(0, 0, 0, 0.8)')\n",
    "            # Intra-dataset traces\n",
    "            traces.append(\n",
    "                go.Scatter(\n",
    "                    x=intra_data['Sample Period'],\n",
    "                    y=intra_data[algo],\n",
    "                    mode='lines+markers',\n",
    "                    name=f\"{algo} Intra\",\n",
    "                    legendgroup=f\"{algo} Intra\",\n",
    "                    line=dict(color=color, width=2, dash='solid'),\n",
    "                    showlegend=show_legend\n",
    "                )\n",
    "            )\n",
    "            # Cross-dataset traces\n",
    "            traces.append(\n",
    "                go.Scatter(\n",
    "                    x=cross_data['Sample Period'],\n",
    "                    y=cross_data[algo],\n",
    "                    mode='lines+markers',\n",
    "                    name=f\"{algo} Cross\",\n",
    "                    legendgroup=f\"{algo} Cross\",\n",
    "                    line=dict(color=color, width=2, dash='dot'),\n",
    "                    showlegend=show_legend\n",
    "                )\n",
    "            )\n",
    "    return traces\n",
    "\n",
    "def create_figure(df_mae_intra, df_mae_cross, df_f1_intra, df_f1_cross, df_nep_intra, df_nep_cross, appliances, algorithms, title):\n",
    "    # Transpose the layout so we have metrics in rows and appliances in columns\n",
    "    fig = make_subplots(\n",
    "        rows=3,  # One row for each metric\n",
    "        cols=len(appliances),  # One column for each appliance\n",
    "        subplot_titles=[f\"{appliance}\" for appliance in appliances],\n",
    "        shared_yaxes=True,\n",
    "        horizontal_spacing=0.03,\n",
    "        vertical_spacing=0.04\n",
    "    )\n",
    "\n",
    "    # Add traces to the figure with the transposed layout\n",
    "    metrics_dfs_intra = {'MAE': df_mae_intra, 'F1': df_f1_intra, 'NEP': df_nep_intra}\n",
    "    metrics_dfs_cross = {'MAE': df_mae_cross, 'F1': df_f1_cross, 'NEP': df_nep_cross}\n",
    "    for row, metric in enumerate(['MAE', 'F1', 'NEP'], start=1):\n",
    "        for col, appliance in enumerate(appliances, start=1):\n",
    "            show_legend = (row == 1 and col == 1)  # Show legend only in the first subplot\n",
    "            df_intra = metrics_dfs_intra[metric]\n",
    "            df_cross = metrics_dfs_cross[metric]\n",
    "            for trace in create_metric_subplot(df_intra, df_cross, metric, [appliance], algorithms, show_legend):\n",
    "                fig.add_trace(trace, row=row, col=col)\n",
    "\n",
    "    # Update layout for the figure\n",
    "    fig.update_layout(\n",
    "        title_text=title,\n",
    "        height=250 * 3,  # Fixed height for 3 metrics\n",
    "        width=200 * len(appliances),  # Adjust the width depending on the number of appliances\n",
    "        font=dict(family=\"Times New Roman\",size=12),\n",
    "        showlegend=True,\n",
    "        legend=dict(\n",
    "            orientation=\"h\",\n",
    "            yanchor=\"bottom\",\n",
    "            y=1.05,  # Adjust legend position\n",
    "            xanchor=\"center\",\n",
    "            x=0.5\n",
    "        ),\n",
    "        template=\"simple_white\"\n",
    "    )\n",
    "    \n",
    "    # Loop through each annotation (subplot title) and set the font size\n",
    "    for annotation in fig.layout.annotations:\n",
    "        annotation.font = dict(\n",
    "        family=\"Times New Roman\",\n",
    "        size=12)\n",
    "        \n",
    "    # Define your y-axis labels\n",
    "    y_axis_labels = ['<b>MAE</b> (W)', '<b>F1-score</b> (-)', '<b>NEP</b> (-)']  # Add more labels as needed\n",
    "    \n",
    "    # Update y-axes with labels\n",
    "    for i, label in enumerate(y_axis_labels, start=1):\n",
    "        fig.update_yaxes(title_text=label, row=i, col=1)\n",
    "\n",
    "    return fig\n",
    "\n",
    "algorithms = ['Seq2Point', 'SGN', 'DAE', 'BiLSTM']\n",
    "title = \"\"\n",
    "algorithm_colors = {\n",
    "    'Seq2Point': 'orange',\n",
    "    'DAE': 'red',\n",
    "    'SGN': 'blue',\n",
    "    'BiLSTM': 'green'}\n",
    "\n",
    "# Call create_figure with both intra and cross dataset validation dataframes\n",
    "fig = create_figure(\n",
    "    df_mae_intra, df_mae_cross,\n",
    "    df_f1_intra, df_f1_cross,\n",
    "    df_nep_intra, df_nep_cross,\n",
    "    df_mae_intra['Appliance'].unique(),\n",
    "    algorithms,\n",
    "    title  # or title_cross, depending on your title preference\n",
    ")\n",
    "\n",
    "fig.show()# Get unique appliances and algorithms for subplot dimensions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
